{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "age = np.array([13, 14, 14,12, 9, 15, 10, 14, 9, 14, 13, 12, 9, 10, 15, 11, \n",
    "                15, 11, 7, 13, 13, 10, 9, 6, 11, 15, 13, 10, 9, 9, 15, 14, \n",
    "                14, 10, 14, 11, 13, 14, 10])\n",
    "price = np.array([2950, 2300, 3900, 2800, 5000, 2999, 3950, 2995, 4500, 2800, \n",
    "                  1990, 3500, 5100, 3900, 2900, 4950, 2000, 3400, 8999, 4000, \n",
    "                  2950, 3250, 3950, 4600, 4500, 1600, 3900, 4200, 6500, 3500, \n",
    "                  2999, 2600, 3250, 2500, 2400, 3990, 4600, 450,4700])/1000.\n",
    "\n",
    "rnorm = np.random.normal\n",
    "runif = np.random.rand\n",
    "\n",
    "from scipy.stats import gamma, norm\n",
    "dgamma = gamma.logpdf\n",
    "dnorm = norm.logpdf\n",
    "\n",
    "def calc_posterior(a, b, t, y=price, x=age):\n",
    "    # Calculate joint posterior, given values for a, b and t\n",
    "\n",
    "    # Priors on a,b\n",
    "    logp = dnorm(a, 0, 10000) + dnorm(b, 0, 10000)\n",
    "    # Prior on t\n",
    "    logp += dgamma(t, 0.001, 0.001)\n",
    "    # Calculate mu\n",
    "    mu = a + b*x\n",
    "    # Data likelihood\n",
    "    logp += sum(dnorm(y, mu, t**-0.5))\n",
    "    \n",
    "    return logp\n",
    "\n",
    "def metropolis(n_iterations, initial_values, prop_var=1):\n",
    "\n",
    "    n_params = len(initial_values)\n",
    "            \n",
    "    # Initial proposal standard deviations\n",
    "    prop_sd = [prop_var]*n_params\n",
    "    \n",
    "    # Initialize trace for parameters\n",
    "    trace = np.empty((n_iterations+1, n_params))\n",
    "    \n",
    "    # Set initial values\n",
    "    trace[0] = initial_values\n",
    "        \n",
    "    # Calculate joint posterior for initial values\n",
    "    current_log_prob = calc_posterior(*trace[0])\n",
    "    \n",
    "    # Initialize acceptance counts\n",
    "    accepted = [0]*n_params\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    \n",
    "        if not i%1000: print('Iteration %i' % i)\n",
    "    \n",
    "        # Grab current parameter values\n",
    "        current_params = trace[i]\n",
    "    \n",
    "        for j in range(n_params):\n",
    "    \n",
    "            # Get current value for parameter j\n",
    "            p = trace[i].copy()\n",
    "    \n",
    "            # Propose new value\n",
    "            if j==2:\n",
    "                # Ensure tau is positive\n",
    "                theta = np.exp(rnorm(np.log(current_params[j]), prop_sd[j]))\n",
    "            else:\n",
    "                theta = rnorm(current_params[j], prop_sd[j])\n",
    "            \n",
    "            # Insert new value \n",
    "            p[j] = theta\n",
    "    \n",
    "            # Calculate log posterior with proposed value\n",
    "            proposed_log_prob = calc_posterior(*p)\n",
    "    \n",
    "            # Log-acceptance rate\n",
    "            alpha = proposed_log_prob - current_log_prob\n",
    "    \n",
    "            # Sample a uniform random variate\n",
    "            u = runif()\n",
    "    \n",
    "            # Test proposed value\n",
    "            if np.log(u) < alpha:\n",
    "                # Accept\n",
    "                trace[i+1,j] = theta\n",
    "                current_log_prob = proposed_log_prob\n",
    "                accepted[j] += 1\n",
    "            else:\n",
    "                # Reject\n",
    "                trace[i+1,j] = trace[i,j]\n",
    "                \n",
    "    return trace, accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "def metropolis_tuned(n_iterations, initial_values, prop_var=1, \n",
    "                     tune_for=None, tune_interval=100):\n",
    "     \n",
    "    n_params = len(initial_values)\n",
    "             \n",
    "    # Initial proposal standard deviations\n",
    "    prop_sd = [prop_var] * n_params\n",
    "     \n",
    "    # Initialize trace for parameters\n",
    "    trace = np.empty((n_iterations+1, n_params))\n",
    "     \n",
    "    # Set initial values\n",
    "    trace[0] = initial_values\n",
    "    # Initialize acceptance counts\n",
    "    accepted = [0]*n_params\n",
    "     \n",
    "    # Calculate joint posterior for initial values\n",
    "    current_log_prob = calc_posterior(*trace[0])\n",
    "     \n",
    "    if tune_for is None:\n",
    "        tune_for = n_iterations/2\n",
    "     \n",
    "    for i in range(n_iterations):\n",
    "     \n",
    "        if not i%1000: print('Iteration %i' % i)\n",
    "     \n",
    "        # Grab current parameter values\n",
    "        current_params = trace[i]\n",
    "     \n",
    "        for j in range(n_params):\n",
    "     \n",
    "            # Get current value for parameter j\n",
    "            p = trace[i].copy()\n",
    "     \n",
    "            # Propose new value\n",
    "            if j==2:\n",
    "                # Ensure tau is positive\n",
    "                theta = np.exp(rnorm(np.log(current_params[j]), prop_sd[j]))\n",
    "            else:\n",
    "                theta = rnorm(current_params[j], prop_sd[j])\n",
    "             \n",
    "            # Insert new value \n",
    "            p[j] = theta\n",
    "     \n",
    "            # Calculate log posterior with proposed value\n",
    "            proposed_log_prob = calc_posterior(*p)\n",
    "     \n",
    "            # Log-acceptance rate\n",
    "            alpha = proposed_log_prob - current_log_prob\n",
    "     \n",
    "            # Sample a uniform random variate\n",
    "            u = runif()\n",
    "     \n",
    "            # Test proposed value\n",
    "            if np.log(u) < alpha:\n",
    "                # Accept\n",
    "                trace[i+1,j] = theta\n",
    "                current_log_prob = proposed_log_prob\n",
    "                accepted[j] += 1\n",
    "            else:\n",
    "                # Reject\n",
    "                trace[i+1,j] = trace[i,j]\n",
    "                 \n",
    "            # Tune every 100 iterations\n",
    "            if (not (i+1) % tune_interval) and (i < tune_for):\n",
    "         \n",
    "                # Calculate aceptance rate\n",
    "                acceptance_rate = (1.*accepted[j])/tune_interval\n",
    "                if acceptance_rate<0.2:\n",
    "                    prop_sd[j] *= 0.9\n",
    "                elif acceptance_rate>0.5:\n",
    "                    prop_sd[j] *= 1.1\n",
    "                accepted[j] = 0\n",
    "                 \n",
    "    return trace[tune_for:], accepted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Slice Sampler with Doubling\n",
    "\n",
    "Modify the slice sampler from Section 2.1 (shown below) to use doubling rather than stepping out to select the horizontal slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform = np.random.uniform\n",
    "\n",
    "def slice(n_iterations, logp, initial_values, w=1, tune=True):\n",
    "    \n",
    "    n_params = len(initial_values)\n",
    "    \n",
    "    # Initialize trace for parameters\n",
    "    trace = np.empty((n_iterations+1, n_params))\n",
    "    \n",
    "    # Set initial values\n",
    "    trace[0] = initial_values\n",
    "    \n",
    "    w_tune = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        if not i%1000: print('Iteration %i' % i)\n",
    "\n",
    "        q = trace[i]\n",
    "        q0 = q.copy()\n",
    "\n",
    "        w = np.resize(w, len(q0))\n",
    "\n",
    "        y = logp(*q0) - np.random.exponential()\n",
    "\n",
    "        # Stepping out procedure\n",
    "        ql = q0.copy()\n",
    "        ql -= uniform(0, w)\n",
    "        qr = q0.copy()\n",
    "        qr = ql + w\n",
    "\n",
    "        yl = logp(*ql)\n",
    "        yr = logp(*qr)\n",
    "\n",
    "        while((y < yl).all()):\n",
    "            ql -= w\n",
    "            yl = logp(*ql)\n",
    "\n",
    "        while((y < yr).all()):\n",
    "            qr += w\n",
    "            yr = logp(*qr)\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            # Sample uniformly from slice\n",
    "            qi = uniform(ql, qr)\n",
    "\n",
    "            yi = logp(*qi)\n",
    "\n",
    "            if yi > y:\n",
    "                q = qi\n",
    "                break\n",
    "            elif (qi > q).all():\n",
    "                qr = qi\n",
    "            elif (qi < q).all():\n",
    "                ql = qi\n",
    "\n",
    "        if tune:\n",
    "            # Tune sampler parameters\n",
    "            w_tune.append(abs(q0 - q))\n",
    "            w = 2 * sum(w_tune, 0) / len(w_tune)\n",
    "\n",
    "        trace[i+1] = q\n",
    "        \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "uniform = np.random.uniform\n",
    "\n",
    "def slice(n_iterations, logp, initial_values, w=1, p=10, tune=True):\n",
    "    \n",
    "    n_params = len(initial_values)\n",
    "    \n",
    "    # Initialize trace for parameters\n",
    "    trace = np.empty((n_iterations+1, n_params))\n",
    "    \n",
    "    # Set initial values\n",
    "    trace[0] = initial_values\n",
    "    \n",
    "    w_tune = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        if not i%1000: print('Iteration %i' % i)\n",
    "\n",
    "        q = trace[i]\n",
    "        q0 = q.copy()\n",
    "\n",
    "        w = np.resize(w, len(q0))\n",
    "\n",
    "        y = logp(*q0) - np.random.exponential()\n",
    "\n",
    "        yl = q0 - w*np.random.random()\n",
    "        yr = yl + w\n",
    "\n",
    "        # Doubling procedure\n",
    "        while (p > 0) and ((y < yl).all() or (y < yr).all()):\n",
    "        \n",
    "            if np.random.random() < 0.5:\n",
    "                \n",
    "                yl -= yr - yl\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                yr += yr - yl\n",
    "                \n",
    "            p -= 1\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            # Sample uniformly from slice\n",
    "            qi = uniform(ql, qr)\n",
    "\n",
    "            yi = logp(*qi)\n",
    "\n",
    "            if yi > y:\n",
    "                q = qi\n",
    "                break\n",
    "            elif (qi > q).all():\n",
    "                qr = qi\n",
    "            elif (qi < q).all():\n",
    "                ql = qi\n",
    "\n",
    "        if tune:\n",
    "            # Tune sampler parameters\n",
    "            w_tune.append(abs(q0 - q))\n",
    "            w = 2 * sum(w_tune, 0) / len(w_tune)\n",
    "\n",
    "        trace[i+1] = q\n",
    "        \n",
    "    return trace"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('bayes_course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "485c2aecfeff35fe97c500045cb91db26354005e32990317d3834cb0213a269e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
